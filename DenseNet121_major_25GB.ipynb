{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet121_major_25GB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nitin-Dwivedi-7/COVID_19-Test-Prediction-Using-CT-scan-Images/blob/main/DenseNet121_major_25GB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb77yZ9fzMG",
        "outputId": "77b9f1fe-f2b0-42c4-f86c-2fd3859af423",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW"
      },
      "source": [
        "#import Keras packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import load_img\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from keras import applications\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.utils import get_file"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JDkZP5cDs03"
      },
      "source": [
        "import pickle \n",
        "file = \"/content/drive/My Drive/data_5K.pkl\"\n",
        "f_o=open(file,\"rb\")\n",
        "data=pickle.load(f_o)\n",
        "f_o.close()\n",
        "file= \"/content/drive/My Drive/labels_5k.pkl\"\n",
        "f_o=open(file,\"rb\")\n",
        "labels=pickle.load(f_o)\n",
        "f_o.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBTLDXpaq1_8"
      },
      "source": [
        "data = np.array(data)\n",
        "labels = np.array(labels)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVgA2wbMq4Aa"
      },
      "source": [
        "img_width, img_height = 256, 256        # Resolution of inputs\n",
        "#train_data_dir = \"train\"           # Folder of train samples\n",
        "#validation_data_dir = \"val\" # Folder of validation samples44\n",
        "#nb_train_samples = 10000                # Number of train samples\n",
        "#nb_validation_samples = 9500            # Number of validation samples\n",
        "batch_size = 64                        # Batch size\n",
        "epochs = 20                # Maximum number of epochs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfQ1oymIq7jX",
        "outputId": "5649594c-f419-447b-f544-6b6d3ca6e1ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load ResNet50\n",
        "model=keras.applications.DenseNet121(weights=\"imagenet\", include_top=False, input_shape=(img_width, img_height, 3))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGPz5cc2rM0z"
      },
      "source": [
        "# Freeze first 15 layers\n",
        "for layer in model.layers[:45]:\n",
        "\tlayer.trainable = False\n",
        "for layer in model.layers[45:]:\n",
        "   layer.trainable = True"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YfNx71vrQpS"
      },
      "source": [
        "x = model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(2, activation=\"softmax\")(x) # 4-way softmax classifier at the end\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW3A0PE0rStQ"
      },
      "source": [
        "nb_train_samples = 2543                # Number of train samples\n",
        "nb_validation_samples = 2261            # Number of validation samples\n",
        "\n",
        "# Initializing the CNN\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJH_xFyprVF1"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical   \n",
        "\n",
        "categorical_labels = to_categorical(labels, num_classes=2)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#X_train1, X_test0, ytrain1, ytest0 = train_test_split(data, categorical_labels, test_size=0.1,\n",
        "                                                    #random_state=random.randint(0,100))\n",
        "X_train1 = data\n",
        "ytrain1 = categorical_labels\n",
        "\n",
        "for index in range(10):\n",
        "  classifier = Model(model.input,predictions)\n",
        "  classifier.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(lr=1e-3, momentum=0.9), metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "X_train, X_test1, ytrain, ytest1 = train_test_split(X_train1, ytrain1, test_size=0.1,\n",
        "                                                    random_state=random.randint(0,100))\n",
        "X_val, X_test, yval, ytest = train_test_split(X_test1, ytest1, test_size=0.5,\n",
        "                                                    random_state=random.randint(0,100))\n",
        "    \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPhxrokdrZBY",
        "outputId": "b514033c-7963-44b0-f30a-d98d3bc4b5fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "    \n",
        "train_datagen = ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            shear_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            horizontal_flip=True)\n",
        "    \n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "training_set = train_datagen.flow(\n",
        "            X_train, ytrain,\n",
        "            batch_size=64)\n",
        "    \n",
        "val_set = val_datagen.flow(\n",
        "            X_val, yval,\n",
        "            batch_size=64)\n",
        "    \n",
        "test_set = test_datagen.flow(\n",
        "            X_test, ytest,\n",
        "            batch_size=64)\n",
        "    \n",
        "X, y = test_set.next()\n",
        "    \n",
        "classifier.fit_generator(\n",
        "            training_set,\n",
        "            steps_per_epoch=20,\n",
        "            epochs=20,\n",
        "            validation_data=val_set,\n",
        "            validation_steps=100)\n",
        "    \n",
        "    #w_file = 'Wilson_vgg16_model_weights_k10.h5'\n",
        "    #classifier.save_weights(w_file)\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-12-f9d3d78a6ee7>:32: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/20\n",
            "20/20 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.8359WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 [==============================] - 17s 848ms/step - loss: 0.5656 - accuracy: 0.8359 - val_loss: 1.1479 - val_accuracy: 0.7062\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 14s 702ms/step - loss: 0.1686 - accuracy: 0.9578\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 15s 753ms/step - loss: 0.0850 - accuracy: 0.9787\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 14s 714ms/step - loss: 0.0317 - accuracy: 0.9898\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 14s 707ms/step - loss: 0.0182 - accuracy: 0.9930\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 14s 710ms/step - loss: 0.0195 - accuracy: 0.9922\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 14s 711ms/step - loss: 0.0188 - accuracy: 0.9953\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 14s 700ms/step - loss: 0.0132 - accuracy: 0.9961\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 14s 702ms/step - loss: 0.0145 - accuracy: 0.9969\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 14s 699ms/step - loss: 0.0076 - accuracy: 0.9977\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 14s 706ms/step - loss: 0.0142 - accuracy: 0.9961\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 14s 707ms/step - loss: 0.0057 - accuracy: 0.9984\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 14s 691ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 14s 705ms/step - loss: 0.0021 - accuracy: 0.9992\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 14s 700ms/step - loss: 0.0061 - accuracy: 0.9977\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 14s 704ms/step - loss: 0.0042 - accuracy: 0.9984\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 14s 720ms/step - loss: 0.0154 - accuracy: 0.9945\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 14s 705ms/step - loss: 0.0029 - accuracy: 0.9992\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 14s 701ms/step - loss: 0.0050 - accuracy: 0.9976\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 14s 707ms/step - loss: 0.0014 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fef95d3db70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAfTyf7YzGCZ",
        "outputId": "a5e64711-1fe5-4dcb-e850-3933ceaaae76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "arr = classifier.evaluate(X,y)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9844    WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0147s vs `on_test_batch_end` time: 0.0580s). Check your callbacks.\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0510 - accuracy: 0.9844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfNSz9h2zIkT",
        "outputId": "b1ba38e7-0c82-45e6-c88c-2f2efac49932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(arr)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.05104001238942146, 0.984375]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_bpNxra6l8W"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6kxGURXrcDw",
        "outputId": "7befe5bc-5dc4-4fea-fb2c-49884feef606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "arr = classifier.predict(X)\n",
        "arr = np.argmax(arr, axis=1)\n",
        "\n",
        "print(arr)\n",
        "print(y)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0\n",
            " 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 0 1 0 1]\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}